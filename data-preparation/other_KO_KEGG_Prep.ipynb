{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088d1e0e-68ab-4a49-acfa-8ec21e82f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78d1e74-bb4d-4763-b706-1203098d1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "KO = pd.read_excel(\"KO_pred_metagenome_unstrat_descrip.xlsx\")\n",
    "Path = pd.read_excel(\"path_abun_unstrat_descrip.xlsx\")\n",
    "df = pd.read_csv(\"cleaned_asv_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e93603-48b5-4797-95e4-aa4d298c7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KO.columns = KO.columns.str.replace(r'^[A-Za-z]_', '', regex=True)\n",
    "Path.columns = Path.columns.str.replace(r'^[A-Za-z]_', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884c9355-63ea-47f7-b765-0810effb078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of Sample_IDs from df\n",
    "sample_ids = df[\"Sample_ID\"].astype(str).tolist()\n",
    "\n",
    "# Check if each column in Path.iloc[:, 2:] is in sample_ids\n",
    "columns_to_keep = [col for col in Path.columns[2:] if col in sample_ids]\n",
    "# Keep the first two columns and the columns that are in sample_ids\n",
    "Path_filtered = Path.iloc[:, :2].join(Path[columns_to_keep])\n",
    "# If you want to update the original Path DataFrame, assign it back to Path\n",
    "Path = Path_filtered\n",
    "\n",
    "# Check if each column in Path.iloc[:, 2:] is in sample_ids\n",
    "columns_to_keep = [col for col in KO.columns[2:] if col in sample_ids]\n",
    "# Keep the first two columns and the columns that are in sample_ids\n",
    "KO_filtered = KO.iloc[:, :2].join(Path[columns_to_keep])\n",
    "# If you want to update the original Path DataFrame, assign it back to Path\n",
    "KO = KO_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8644d8ad-5d11-4828-9e78-d89f1184283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transpose the Path DataFrame, make the first row the column names, and set the index as Sample_ID\n",
    "path_transpose = Path.T  # Transpose the DataFrame\n",
    "\n",
    "# Set the first row as column names\n",
    "path_transpose.columns = path_transpose.iloc[0]\n",
    "\n",
    "# Drop the first row now that it is set as column names\n",
    "path_transpose = path_transpose.drop(path_transpose.index[0])\n",
    "\n",
    "# Reset the index, so you can modify it and create a Sample_ID column\n",
    "path_transpose = path_transpose.reset_index()\n",
    "\n",
    "# Rename the current index (which was the previous columns in Path) to 'Sample_ID'\n",
    "path_transpose = path_transpose.rename(columns={'index': 'Sample_ID'})\n",
    "\n",
    "# Step 1: Ensure Sample_ID is of the same type in both DataFrames\n",
    "path_transpose['Sample_ID'] = path_transpose['Sample_ID'].astype(str)\n",
    "df['Sample_ID'] = df['Sample_ID'].astype(str)\n",
    "\n",
    "# Step 2: Add the last 13 columns from df to path_transpose using merge\n",
    "path_transpose = path_transpose.merge(df[['Sample_ID'] + df.columns[-13:].tolist()], on='Sample_ID', how='left')\n",
    "\n",
    "# The `merge` operation adds the last 13 columns from df to path_transpose, matching on 'Sample_ID'\n",
    "Path = path_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05be1ed8-26d5-4aed-b115-376abbe8fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transpose the KO DataFrame, make the first row the column names, and set the index as Sample_ID\n",
    "ko_transpose = KO.T  # Transpose the DataFrame\n",
    "\n",
    "# Set the first row as column names\n",
    "ko_transpose.columns = ko_transpose.iloc[0]\n",
    "\n",
    "# Drop the first row now that it is set as column names\n",
    "ko_transpose = ko_transpose.drop(ko_transpose.index[0])\n",
    "\n",
    "# Reset the index, so you can modify it and create a Sample_ID column\n",
    "ko_transpose = ko_transpose.reset_index()\n",
    "\n",
    "# Rename the current index (which was the previous columns in KO) to 'Sample_ID'\n",
    "ko_transpose = ko_transpose.rename(columns={'index': 'Sample_ID'})\n",
    "\n",
    "# Step 1: Ensure Sample_ID is of the same type in both DataFrames\n",
    "ko_transpose['Sample_ID'] = ko_transpose['Sample_ID'].astype(str)\n",
    "df['Sample_ID'] = df['Sample_ID'].astype(str)\n",
    "\n",
    "# Step 2: Add the last 13 columns from df to ko_transpose using merge\n",
    "ko_transpose = ko_transpose.merge(df[['Sample_ID'] + df.columns[-13:].tolist()], on='Sample_ID', how='left')\n",
    "\n",
    "# The `merge` operation adds the last 13 columns from df to ko_transpose, matching on 'Sample_ID'\n",
    "KO = ko_transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b95824-824c-4711-b771-47f81001f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_numeric = Path.iloc[1:,1:-13]\n",
    "KO_numeric = KO.iloc[1:,1:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e612ca0e-40a8-472b-aa11-ac6ec5ba8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for columns in Path_numeric that are entirely NaN or 0\n",
    "cols_to_drop_path = Path_numeric.columns[(Path_numeric.isna().all() | (Path_numeric == 0).all())]\n",
    "\n",
    "# Checking for columns in KO_numeric that are entirely NaN or 0\n",
    "cols_to_drop_ko = KO_numeric.columns[(KO_numeric.isna().all() | (KO_numeric == 0).all())]\n",
    "\n",
    "# Dropping the columns\n",
    "Path_numeric = Path_numeric.drop(columns=cols_to_drop_path)\n",
    "Path = Path.drop(columns=cols_to_drop_path)\n",
    "KO_numeric = KO_numeric.drop(columns=cols_to_drop_ko)\n",
    "KO = KO.drop(columns=cols_to_drop_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f645f3-9dd2-4a6b-a044-e105bfdbc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "KO.to_csv(\"KO_df.csv\", index=False)\n",
    "Path.to_csv(\"Path_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c4429a-a1e6-4f57-a95b-f1d01dc3a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate detection limit and impute zeros with pseudo values\n",
    "def impute_pseudo_values(df):\n",
    "    imputed_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        total_sum = row.sum()\n",
    "        if total_sum > 0:\n",
    "            detection_limit = 1 / total_sum\n",
    "            lower_bound = 0.1 * detection_limit\n",
    "            upper_bound = detection_limit\n",
    "            imputed_row = row.apply(\n",
    "                lambda x: np.random.uniform(lower_bound, upper_bound) if x == 0 else x\n",
    "            )\n",
    "            imputed_rows.append(imputed_row)\n",
    "        else:\n",
    "            imputed_rows.append(row)\n",
    "    \n",
    "    # Create a new DataFrame from the imputed rows\n",
    "    imputed_df = pd.DataFrame(imputed_rows, columns=df.columns, index=df.index)\n",
    "    return imputed_df\n",
    "\n",
    "# Function to perform CLR transformation\n",
    "def clr(df):\n",
    "    numeric_cols = df.columns\n",
    "    clr_values = []\n",
    "    for index, row in df.iterrows():\n",
    "        numeric_row = row[numeric_cols].astype(float)\n",
    "        if (numeric_row <= 0).any():\n",
    "            raise ValueError(f\"Non-positive values found in row {index}.\")\n",
    "        geom_mean = gmean(numeric_row)\n",
    "        clr_row = np.log(numeric_row / geom_mean)\n",
    "        clr_values.append(clr_row)\n",
    "    clr_df = pd.DataFrame(clr_values, columns=numeric_cols)\n",
    "    return clr_df\n",
    "\n",
    "# Function to process Path_numeric and KO_numeric, and add back the original columns and rows\n",
    "def process_data_with_originals(df_original, df_numeric):\n",
    "    # Step 1: Impute pseudo values\n",
    "    df_numeric_imputed = impute_pseudo_values(df_numeric.copy())\n",
    "    \n",
    "    # Step 2: CLR transformation\n",
    "    df_numeric_clr = clr(df_numeric_imputed)\n",
    "    \n",
    "     # Step 3: Add back the first row and other columns and rows from the original dataframe\n",
    "    df_combined = pd.concat([df_original.iloc[:1, :], df_numeric_clr], axis=0)  # Adding back the first row\n",
    "    # Drop 'Sample_ID' if present\n",
    "    if 'Sample_ID' in df_combined.columns:\n",
    "        df_combined = df_combined.drop(columns=[\"Sample_ID\"], axis=1)\n",
    "    df_combined = pd.concat([df_original.iloc[:, :1], df_combined, df_original.iloc[:, -13:]], axis=1)  # Adding back the other columns\n",
    "    \n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ef8ca89-483c-48a1-9dc6-5f55bfd4ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying to Path_numeric and KO_numeric\n",
    "Path_combined = process_data_with_originals(Path, Path_numeric)\n",
    "KO_combined = process_data_with_originals(KO, KO_numeric)\n",
    "Path_combined.to_csv(\"Path_CLR_df.csv\", index = False)\n",
    "KO_combined.to_csv(\"KO_CLR_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3eb6f0-230c-44aa-9f8c-e1e9561fa76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
